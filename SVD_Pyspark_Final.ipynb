{"cells":[{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer,NGram,OneHotEncoderEstimator, VectorAssembler, PCA, StringIndexer\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.feature import StopWordsRemover\nfrom pyspark.sql.types import *\nfrom nltk.stem.porter import *\nfrom pyspark.ml import Pipeline \nfrom pyspark.sql.functions import col, size\nimport pyspark.sql.functions as F\n\nimport nltk\nnltk.download('wordnet')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#DIMENSIONALITY REDUCTION IN SVD\nfrom sklearn.decomposition import TruncatedSVD\nfrom scipy.sparse import csr_matrix\nimport numpy as np\nimport pandas as pd\n\n#n is the number of components\nn=450 \n\n#Read preprocessed data\ntrain_dt=spark.table(\"train_vector_assembled\")\ntrain_final_df =train_dt.select(\"features\",\"label\")\n\ntest_dt=spark.table(\"test_vector_assembled\")\ntest_final_df =test_dt.select(\"features\",\"label\")\n\n\n#Convert to appropriate format for sklearn\ntrain_features_pd = train_final_df.toPandas()\ntrain_features_series = train_features_pd['features'].apply(lambda x : np.array(x.toArray())).as_matrix().reshape(-1,1)\ntrain_features = np.apply_along_axis(lambda x : x[0], 1, train_features_series)\n\n\ntest_features_pd = test_final_df.toPandas()\ntest_features_series = test_features_pd['features'].apply(lambda x : np.array(x.toArray())).as_matrix().reshape(-1,1)\ntest_features = np.apply_along_axis(lambda x : x[0], 1, test_features_series)\n\n#Run SVD\nsvd = TruncatedSVD(n_components=n, n_iter=7, random_state=42)\ntrain_pca_features_arr = svd.fit_transform(train_features)\ntest_pca_features_arr = svd.transform(test_features)\n\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#Convert sklearn format back to Spark DataFrame\nfrom pyspark.sql.types import StructType, StructField, LongType\n\ndef with_column_index(sdf): \n    new_schema = StructType(sdf.schema.fields + [StructField(\"ColumnIndex\", LongType(), False),])\n    return sdf.rdd.zipWithIndex().map(lambda row: row[0] + (row[1],)).toDF(schema=new_schema)\n\n\n  \n#Returns a df to be passed to model for training, col: pca_features, variety_idx\n\n\nassembler = VectorAssembler(\n    inputCols=[\"pca[{0}]\".format(i) for i in range(n)], \n    outputCol=\"pca_features\")\n\n\n\ndef refactor_df(original_df, svd_output):\n  svd_output = svd_output.astype('float64')\n  \n  svd_pd = pd.DataFrame(svd_output)\n#   svd_pd['pca'] = svd_pd.values.tolist()\n  svd_pd['pca'] = svd_pd.values.tolist()\n\n  \n  svd_df = spark.createDataFrame(svd_pd)\n  svd_df = svd_df.select(\"*\").alias('pca').select('pca')\n  \n  svd_df =assembler.transform(svd_df.select(\n    \"*\", *(svd_df[\"pca\"].getItem(i) for i in range(n))\n  ))\n#   svd_df.count()\n#   svd_df = svd_df.select(\"pca\")\n  \n  df1_ci = with_column_index(original_df)\n  df2_ci = with_column_index(svd_df)\n  new_df = df1_ci.join(df2_ci, df1_ci.ColumnIndex == df2_ci.ColumnIndex, 'inner').select(\"pca_features\",\"label\")\n  \n  return new_df\n\n\n\ntrain_df_svd = refactor_df(train_final_df,train_pca_features_arr)\ntest_df_svd = refactor_df(test_final_df,test_pca_features_arr)\n\n\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#Save the data processed with SVD to a table\ntrain_df_svd.write.saveAsTable(\"train_svd_600_n\")\ntest_df_svd.write.saveAsTable(\"test_svd_600_n\")"],"metadata":{},"outputs":[],"execution_count":4}],"metadata":{"name":"Wine Variety Final","notebookId":1261577949778699},"nbformat":4,"nbformat_minor":0}
